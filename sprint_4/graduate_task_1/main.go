package main

import (
	"bufio"
	"fmt"
	"os"
	"strconv"
	"strings"
)

// https://contest.yandex.ru/contest/24414/run-report/106354869/

// После последней оптимизации время всего 0.55 сек! (уменьшили в 4 раза)))

/**
  -- ПРИНЦИП РАБОТЫ --
  Создаем индекс поиска в виде ассоциативного массива: ключ - уникальные слова из документов, значение - слайс структур вхождений слова в каждый документ (индекс документа - кол-во вхождений).
  Далее в цикле для каждого запроса: собираем множество уникальных слов из запроса.
  Итерируемся по словам из множества и собираем слайс структур вхождений слов из запроса в разные документы.
  Итерируемся по полученному слайсу и строим слайс релевантности: индекс ячейки - индекс документа, значение по индексу - кол-во вхождений слова в документ.
  В цикле до 5 раз выбираем наиболее релевантный документ из слайса, "обнуляя" найденный.

  -- ДОКАЗАТЕЛЬСТВО КОРРЕКТНОСТИ --
  В результате построения индекса получаем ассоциативный массив, в котором ключ - уникальные слова из документов, значение - слайс структур вхождений слова в каждый документ.
  В силу специфики устройства map (все ключи являются уникальными) мы уверены, что индекс составлен по всем словам документа без повторов.

  Далее для каждого запроса строим множества  слов на основе map, где значение пустая структура (идиоматичный способ построения set для go).
  Опять же в силу специфики устройства map (все ключи являются уникальными) мы уверены, что множество не содержит повторяющихся значений.

  На основе указанного выше индекса (по всем документам) и множества слов конкретного запроса рассчитываем релевантность запроса.
  Итерируемся по множеству слов и строим слайс структур вхождений слов запроса в документы.
  Т.к. индекс для каждого слова документа содержит корректный массив вхождений (указано выше благодаря устройству map), мы получим корректный массив вхождений для каждого слова запроса.
  На данном этапе нам необходимо определить наибольший индекс документа, в котором встречались слова из запроса.

  Строим слайс релевантности размером n + 1, где n = максимальный индекс документа в котором встречалось слово из запроса.
  Итерируемся по слайсу вхождений, в слайс релевантности в соответствующий индекс добавляем кол-во вхождений данного слова.
  Если ячейка слайса непустая - добавляем соответствующие вхождения.
  Данная операция будет корректна, т.к. индекс документа принадлежит [1;10^4].
  Итерируемся по слайсу релевантности удаляя пустые вхождения вида {0, 0}.
  На слайсе релевантности благодаря гарантиям условия задачи мы получаем плюсы устройства map - "обращение по индексу к ячейке" с записью результата в значение,
  но значительно более дешевую итерацию по слайсу, нежели по map в дальнейших операциях.

  Далее нам необходимо выбрать 5 или менее документов с наибольшей релевантность.
  Итерируемся по слайсу релевантности n раз (до 5 раз) каждый раз будем находить наиболее релевантный документ и затем "обнулять его",
  проставляя значения {-1, -1}, что гарантирует нам, что данный элемент однозначно не будет выбран наиболее релевантным в следующей итерации (индекс документа и кол-во вхождений не могут быть отрицательными).
  Совокупность гарантий устройства ассоциативного массива и способа отбора релевантных документов гарантируют корректность алгоритма.

  -- ВРЕМЕННАЯ СЛОЖНОСТЬ --
  Создание индекса - линейная сложность O(n * y) =  где n - кол-во документов, y - кол-во слов в документе.
  Длина текста не превышает 1000 символов. Можем принять их за константу.
  Соответственно в среднем сложность построения индекса заставит O(n) - где n кол-во слов во всех документах.

  Вывод ответа - i последовательных операций ниже (для каждого запроса), где i - кол-во запросов:
  Источником всех операций является текст запроса, длина которого фиксирована в 100 символов.
  Все операции ниже так или иначе работают с запросом. Вложенные циклы, за исключением последней операции отсутствуют.
  Однако кол-во итераций внешнего цикла фиксировано - не более 5, поэтому вложенностью можно пренебречь.
  		 O(k) - создание set уникальных слов из запроса (k - кол-во слов в запросе)
  		 O(s) - создание слайса структур вхождений слов из запроса в документы (s - кол-во слов в множестве, полученном выше)
		 O(m) - создание слайса релевантности по уникальным словам (m - размер слайса вхождений выше)
		 O(l) - удаление из слайса релевантности значений вида {0,0} (l - кол-во элементов в слайсе релевантности выше)
  	     O(h * u) - поиск в слайсе структур релевантности до 5 самых больших вхождений (h <= 5) можно принять за линейное время
		 O(k + s + m + + l + (h * u)) = O(g) - сумма линейных последовательных операций выше c запросом

         O(i * g) = обработка всех запросов, где i - кол-во запросов, g - сумма линейных операций с запросом фиксированной длины.
  Т.к. запрос не может быть более 100 символов, операции со словами запросов можно принять за константу.
  Соответственно в среднем сложность операций с запросами составит O(e), где e - кол-во слов во всех запросах.

  В общем случае временная сложность будет линейной O(n) + O(e) где O(n) построение индекса в зависимости от кол-ва слов в документах
  и O(e) где e - кол-во слов во всех запросах

  -- ПРОСТРАНСТВЕННАЯ СЛОЖНОСТЬ --
  O(n) доп. памяти на построение индекса
  O(s + m + l) доп. памяти где s - кол-во уникальных слов, m - слайс вхождений и l - слайс структур с релевантностью документов для запроса
*/

const ResultLimit = 5

type occurrence struct {
	docIdx int
	count  int
}

type score struct {
	docIdx int
	weight int
}

/*
 * Строим поисковый индекс по документам.
 * Для каждого уникального слова строим map в которой ключ - слово, значение - слайс структур вхождений слова в каждый документ.
 * Получим индекс, в котором находится информация о всех уникальных словах и частоте вхождений
 * в тот или иной документ.
 */
func buildIndex(input []string, docCount int) map[string][]occurrence {
	var index = make(map[string][]occurrence)
	for i := 1; i < docCount+1; i++ {
		words := strings.Split(input[i], " ")
		for a := 0; a < len(words); a++ {
			occurrences, found := index[words[a]]
			if !found || occurrences[len(occurrences)-1].docIdx != i {
				index[words[a]] = append(occurrences, occurrence{i, 1})
			} else {
				occurrences[len(occurrences)-1].count++
			}
		}
	}

	return index
}

/*
 * Получаем set уникальных слов из соответствующего запроса
 * O(k)
 */
func getUniqueWords(input string) map[string]struct{} {
	words := strings.Split(input, " ")
	uniqueWords := make(map[string]struct{})

	for a := 0; a < len(words); a++ {
		uniqueWords[words[a]] = struct{}{}
	}

	return uniqueWords
}

/*
 * Рассчитываем релевантность запроса относительно документа по index map, построенной ранее.
 */
func calcRelevance(uniqueWords map[string]struct{}, index map[string][]occurrence) []score {
	// Итерируемся по set уникальных слов из запроса
	// если слово встречается в индексе - заполняем слайс вхождений слов из запроса в документы
	// Тут же нам надо определить максимальный номер документа в котором встречается слово
	// Он потребуется для составления слайса релевантности
	// O(s)
	maxIdx := -1
	var queryWords []occurrence
	for k, _ := range uniqueWords {
		if occurrences, found := index[k]; found {
			if occurrences[len(occurrences)-1].docIdx > maxIdx {
				maxIdx = occurrences[len(occurrences)-1].docIdx
			}
			queryWords = append(queryWords, occurrences...)
		}
	}

	// Строим слайс релевантности для каждого документа
	// Размер слайса - максимальный номер документа в котором встречается слово (+1)
	// Номер документа записываем в соответствующую ячейку слайса по индексу.
	// Если в ячейке уже есть число - увеличиваем его на число вхождений данного слова
	// O(m)
	relevance := make([]score, maxIdx+1)
	for _, q := range queryWords {
		if relevance[q.docIdx].docIdx == 0 {
			relevance[q.docIdx].docIdx = q.docIdx
			relevance[q.docIdx].weight = q.count
		} else {
			relevance[q.docIdx].weight = relevance[q.docIdx].weight + q.count
		}
	}

	// Слайс релевантности может включать значения вида {0, 0}
	// Необходимо исключить такие значения, чтобы рассчитать число элементов для выборки
	// O(l)
	var scores []score
	for _, v := range relevance {
		if v.docIdx != 0 {
			scores = append(scores, v)
		}
	}

	n := ResultLimit
	if len(scores) < ResultLimit {
		n = len(scores)
	}

	// Ищем 5 наиболее релевантных документов
	// Каждый найденный документ "обнуляем" для следующей итерации
	// O(h * u)
	maxScores := make([]score, 0, n)
	h := 0
	for h < n {
		maxScore := score{
			docIdx: -1,
			weight: -1,
		}
		maxIdx := -1
		for i := 0; i < len(scores); i++ {
			if scores[i].weight > maxScore.weight || (scores[i].weight == maxScore.weight && scores[i].docIdx < maxScore.docIdx) {
				maxScore = scores[i]
				maxIdx = i
			}
		}
		maxScores = append(maxScores, maxScore)
		scores[maxIdx] = score{-1, -1}
		h++
	}

	return maxScores
}

/*
 * Готовим вывод с учетом ограничения на кол-во релевантных документов к выводу.
 */
func printResult(scores []score) {
	var p strings.Builder
	for k := 0; k < len(scores); k++ {
		if k == len(scores)-1 {
			p.WriteString(strconv.Itoa(scores[k].docIdx))
		} else {
			p.WriteString(strconv.Itoa(scores[k].docIdx) + " ")
		}
	}

	fmt.Println(p.String())
}

func main() {
	input := getInputData()
	docCount, _ := strconv.Atoi(input[0])

	//Строим индекс поиска в виде map по всем документам
	index := buildIndex(input, docCount)

	//Для каждого запроса вычисляем релевантность и выводим ответ
	for i := docCount + 2; i < len(input); i++ {
		uniqueWords := getUniqueWords(input[i])
		scores := calcRelevance(uniqueWords, index)
		printResult(scores)
	}
}

func getInputData() []string {
	input, _ := os.Open("input.txt")
	defer input.Close()

	scanner := bufio.NewScanner(input)
	scanner.Split(bufio.ScanLines)

	var s []string
	for scanner.Scan() {
		bufStr := scanner.Text()
		s = append(s, bufStr)
	}

	return s
}
